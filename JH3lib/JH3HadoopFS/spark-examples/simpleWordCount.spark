// First method (Datasets)
case class InData(line: String)
case class OutData(word: String)
val dataDF = sc.textFile("dataset.json").toDF("line")
dataDF.write.text("h3://mybucket/dir1/data.txt")
val readDF = spark.read.text("h3://mybucket/dir1/data.txt").toDF("line")
val wordsDF = readDF.as[InData].flatMap(l => l.line.split(' ').map(OutData))
val wordCountDF = wordsDF.groupBy("word").count()
wordCountDF.show()

// alternative (explode is deprecated)
val dataDF = sc.textFile("dataset.json").toDF("line")
dataDF.write.text("h3://mybucket/dir1/data2.txt")
val readDF = spark.read.text("h3://mybucket/dir1/data2.txt").toDF("line")
val wordsDF = readDF.explode("line", "word")((line: String) => line.split(" "))
val wordCountDF = wordsDF.groupBy("word").count()
wordCountDF.show()
